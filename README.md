# TP Fouille de textes et TALN

L'objectif de ce TP est de vous faire découvrir la fouille de textes et les techniques de Traitement Automatique du Langage Naturel afin de résoudre des problèmes concrêts rencontrés en entreprise.

Pour réaliser ce TP, nous suivrons un cas d'usage fil conducteur  dans l'ordre les X modules du TP : 

1. Découverte de l'environnement et des données
2. Lecture des données, OCR et normalisation des textes
3. Extraction d'informations pertinentes
4. Catégorisation des données
5. Indexation des données et mise en place de visualisations pertinentes

## Description du cas d'usage

Nous avons à notre disposition une multitude de documents (pdf, word, ppt) éparpillées dans des répertoires ainsi que des données issues de bases de données telles nous pourrions les avoir en entreprise.

Le problème qui se pose est le suivant : comment exploiter ces données hétérogènes pour répondre aux questions suivantes :
- trouver toutes les références à des produits ;
- catégoriser les informations selon des catégories métiers ;
- rechercher des documents efficacement ;
- mettre en place des indicateurs utilisables par le métier.

## 1. Découverte de l'environnement et des données

Utilisation de CommonCrawlDocument

## 2. Lecture des données et normalisation des textes

Python / notebook ou Dataiku ?

## 3. Extraction d'informations pertinentes

Python / notebook ou Dataiku ?

## 4. Catégorisation des données

Utilisation de modèles Huggingface : zeroshot learning ou entraînement / inference

## 5. Indexation des données et mise en place de visualisations pertinentes

Elasticsearch / Kibana ou OpenSearch / Dashboards