{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 4. - Classification documentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classification documentaire consiste à catégoriser un texte ou un document dans une ou plusieurs catégorie prédéfinie. \n",
    "\n",
    "Les principes généraux sont les mêmes que ceux vus pour la classification d'images, à la différence que les données d'entrée ne sont pas des images mais du texte.\n",
    "\n",
    "Il existe de nombreux algorithmes de classification de documents. Cependant, ceux qui fonctionnent le mieux à ce jour sont les systèmes reposant sur une représentation vectorielle des documents car celle-ci _encode_ le sens des informations et évite les écueils liés aux grandes variabilités syntaxiques que l'on retrouve dans les textes.\n",
    "\n",
    "Pour cette partie, nous allons utiliser un modèle pré-entraîné de type BERT. \n",
    "\n",
    "Nouveauté par rapport au cours, nous allons étudier un modèle de classification _zero shot_ qui permet de spécifier les étiquettes lors de la phase d'inférence et non plus à l'entraînement. Ce nouveau type de modèle facilite grandement la mise en place d'un nouveau classifieur. La contre partie est des performances moindre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des bibliothèques logicielles et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification # Tokenizer & classification de textes à partir de transformers.\n",
    "from transformers import pipeline # Mise en oeuvre de chaînes de traitement.\n",
    "from datasets import load_dataset # Intégration de jeux de données.\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PRE_TRAINED_MODEL_NAME = \"BaptisteDoyen/camembert-base-xnli\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement du jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le cas d'usage que nous allons résoudre est la classification de d'opinions d'avis utilisateur sur des livres.\n",
    "\n",
    "Pour ce TP, l'opinion est binaire : **positif** ou **négatif**.\n",
    "\n",
    "Première étape : récupération du jeu de données. Pour cela, exécuter la commande suivante qui permettre de télécharger les données depuis Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Abirate--french_book_reviews-ccadc0dbf3873cdd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/Abirate--french_book_reviews to /home/vincent/.cache/huggingface/datasets/Abirate___json/Abirate--french_book_reviews-ccadc0dbf3873cdd/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f634b5f30194b31b733d1d5e8116bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba08d3d1f974f6fbcdd7e0d493551f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d481ad2f5e4d5fb7730d0a2148d2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002699c167f14b7295cc76a69913763d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/vincent/.cache/huggingface/datasets/Abirate___json/Abirate--french_book_reviews-ccadc0dbf3873cdd/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a39e841e7834404bf44d73cf25a5686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "french_book_review_dataset = load_dataset(\"Abirate/french_book_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement du modèle pré-entraîné et configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, chargeons un modèle pré-entraîné à la classification _zero shot_. Le modèle que nous utilisons est un modèle ayant été appris sur la partie française du jeu de donées _XNLI_ (https://github.com/facebookresearch/XNLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading or downloading model BaptisteDoyen/camembert-base-xnli\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading or downloading model {}\".format(PRE_TRAINED_MODEL_NAME))\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME) # Chargement du tokenizer.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME) # Chargement du modèle pour la classification de séquences de textes.\n",
    "classifier = pipeline(\"zero-shot-classification\", model=PRE_TRAINED_MODEL_NAME) # Réalisation de la chaîne de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le modèle est chargé, spécifions les catégories (_labels_) dans lesquels nous allons classer nos documents : **positif**, **négatif**.\n",
    "\n",
    "Nous spécifions aussi une hypothèse pour mieux catégoriser le document. L'idée est d'estimer la probabilité conditionnelle `P(label|hypothèse)` pour chaque texte puis de sélectionner le label qui maximise cette probabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"négatif\", \"positif\"] # catégories.\n",
    "hypothesis_template = \"L'avis sur ce livre est plutôt {}.\" # Hypothèse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du format du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['book_title', 'author', 'reader_review', 'rating', 'label'],\n",
      "        num_rows: 9658\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "total = len(french_book_review_dataset.data[\"train\"])\n",
    "print(french_book_review_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation du classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer notre classifieur, nous allons utiliser la métrique simple de la précision : `nombre de bonnes prédictions / # total de prédictions`.\n",
    "\n",
    "Il est aussi possible d'utiliser d'autres métriques comme le score `F1` ou d'autres que vous avez vu aux TPs précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(dataset, model):\n",
    "    \"\"\"\n",
    "    Fonction prenant en paramètre le jeu de données et un modèle et retournant la précision des prédictions.\n",
    "    \"\"\"\n",
    "    predictions, groudtruth = [], []\n",
    "    correct_classification_count = 0\n",
    "    accuracy = 0.0\n",
    "    for text, label in tqdm(zip(dataset.data[\"train\"][\"reader_review\"], dataset.data[\"train\"][\"label\"]), total=len(dataset.data[\"train\"])):\n",
    "        if(text.as_py() is None or text.as_py() == \"\"):\n",
    "            continue\n",
    "        \n",
    "        prediction = model(text.as_py(), labels, multi_label=False, hypothesis_template=hypothesis_template)\n",
    "        predicted_label = prediction[\"labels\"][0]\n",
    "        predicted_label_int = 1 if predicted_label == \"positif\" else 0\n",
    "        predictions.append(predicted_label)\n",
    "        groudtruth.append(label)\n",
    "        if (predicted_label_int == label.as_py()):\n",
    "            correct_classification_count += 1\n",
    "        accuracy = correct_classification_count / len(predictions)\n",
    "        if(len(predictions) % 100 == 0):\n",
    "            print(\"Accuracy after {} texts processed = {}\".format(len(predictions), accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lançons la classification sur l'ensemble des documents. Toutes les 100 prédictions, la fonction `evaluate_classifier` retourne la précision. \n",
    "\n",
    "Si le temps de traitement est trop long, vous pouvez arrêter le processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476220489390467ab036aa4b56e643b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 100 texts processed = 0.72\n",
      "Accuracy after 200 texts processed = 0.72\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document classification.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document%20classification.ipynb#ch0000019?line=0'>1</a>\u001b[0m accuracy \u001b[39m=\u001b[39m evaluate_classifier(french_book_review_dataset, classifier)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document%20classification.ipynb#ch0000019?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(accuracy)\n",
      "\u001b[1;32m/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document classification.ipynb Cell 18'\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document%20classification.ipynb#ch0000017?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m(text\u001b[39m.\u001b[39mas_py() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m text\u001b[39m.\u001b[39mas_py() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document%20classification.ipynb#ch0000017?line=9'>10</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document%20classification.ipynb#ch0000017?line=11'>12</a>\u001b[0m prediction \u001b[39m=\u001b[39m model(text\u001b[39m.\u001b[39;49mas_py(), labels, multi_label\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, hypothesis_template\u001b[39m=\u001b[39;49mhypothesis_template)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document%20classification.ipynb#ch0000017?line=12'>13</a>\u001b[0m predicted_label \u001b[39m=\u001b[39m prediction[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincent/dev/ENSEIGNEMENTS/taln-course/notebooks/4-Document%20classification.ipynb#ch0000017?line=13'>14</a>\u001b[0m predicted_label_int \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m predicted_label \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpositif\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/pipelines/zero_shot_classification.py:181\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to understand extra arguments \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(sequences, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1025\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1048\u001b[0m, in \u001b[0;36mChunkPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1046\u001b[0m all_outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m   1047\u001b[0m \u001b[39mfor\u001b[39;00m model_inputs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params):\n\u001b[0;32m-> 1048\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1049\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(model_outputs)\n\u001b[1;32m   1050\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(all_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:943\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    942\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 943\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    944\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    945\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/pipelines/zero_shot_classification.py:200\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m sequence \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    199\u001b[0m model_inputs \u001b[39m=\u001b[39m {k: inputs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmodel_input_names}\n\u001b[0;32m--> 200\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m    202\u001b[0m model_outputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcandidate_label\u001b[39m\u001b[39m\"\u001b[39m: candidate_label,\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m: sequence,\n\u001b[1;32m    205\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m: inputs[\u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    206\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moutputs,\n\u001b[1;32m    207\u001b[0m }\n\u001b[1;32m    208\u001b[0m \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1205\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1205\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1206\u001b[0m     input_ids,\n\u001b[1;32m   1207\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1208\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1209\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1210\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1211\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1212\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1213\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1214\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1215\u001b[0m )\n\u001b[1;32m   1216\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1217\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:847\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    838\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    840\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    841\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    842\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    845\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    848\u001b[0m     embedding_output,\n\u001b[1;32m    849\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    850\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    851\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    852\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    853\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    854\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    855\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    856\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    857\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    858\u001b[0m )\n\u001b[1;32m    859\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    860\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:523\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    514\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    515\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[1;32m    522\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    524\u001b[0m         hidden_states,\n\u001b[1;32m    525\u001b[0m         attention_mask,\n\u001b[1;32m    526\u001b[0m         layer_head_mask,\n\u001b[1;32m    527\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    528\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    529\u001b[0m         past_key_value,\n\u001b[1;32m    530\u001b[0m         output_attentions,\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    533\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    534\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:450\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    447\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    448\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 450\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    452\u001b[0m )\n\u001b[1;32m    453\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    455\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/pytorch_utils.py:241\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:462\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> 462\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    463\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:361\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 361\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    362\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    363\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/ENSEIGNEMENTS/taln-course/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = evaluate_classifier(french_book_review_dataset, classifier)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont encourageants, d'autant qu'aucune phase d'apprentissage n'a été réalisé préalablement.\n",
    "\n",
    "L'avantage de cette approche est qu'elle permet de définir beaucoup plus rapidement de nouvelles chaînes de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test sur des textes écrits par vous !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécute le bloc de code ci-dessous pour saisir un texte. Le programme vous indiquera si ce qui est écrit est positif ou négatif !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_widget = widgets.Text( description='Votre avis?' )\n",
    "output_widget = widgets.Label(value='En attente...' )\n",
    "\n",
    "vb=widgets.VBox([user_input_widget, output_widget])\n",
    "\n",
    "\n",
    "def callback(wdgt):\n",
    "    prediction = classifier(user_input_widget.value, labels, multi_label=False, hypothesis_template=hypothesis_template)\n",
    "    predicted_label = prediction[\"labels\"][0]\n",
    "    score = prediction[\"scores\"][0]\n",
    "    output_widget.value=\"{} avec un score de {}\".format(predicted_label, score)\n",
    "\n",
    "\n",
    "user_input_widget.on_submit(callback)\n",
    "\n",
    "display(vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Création d'un nouveau classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : modifier le code ci-dessous pour classer les documents selons les catégories suivantes : `subjectif`, `objectif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_category_label = [\"objectif\", \"subjectif\"] # TODO\n",
    "book_category_hypothesis_template = \"L'avis sur ce livre est {}.\" # TODO\n",
    "\n",
    "user_input_widget = widgets.Text( description='Votre avis?' )\n",
    "output_widget = widgets.Label(value='En attente...' )\n",
    "\n",
    "vb=widgets.VBox([user_input_widget, output_widget])\n",
    "\n",
    "\n",
    "def callback(wdgt):\n",
    "    prediction = classifier(user_input_widget.value, book_category_label, multi_label=False, hypothesis_template=book_category_hypothesis_template)\n",
    "    predicted_label = prediction[\"labels\"][0]\n",
    "    score = prediction[\"scores\"][0]\n",
    "    output_widget.value=\"{} avec un score de {}\".format(predicted_label, score)\n",
    "\n",
    "\n",
    "user_input_widget.on_submit(callback)\n",
    "\n",
    "display(vb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c80391f1238b45698e2dd8cb3d47fccae97762fdc20421908c45a9aed68776a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
