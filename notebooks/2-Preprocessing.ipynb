{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1. - Pré-traitements du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons étudier les principes de base du pré-traitement de textes.\n",
    "\n",
    "L'objectif du pré-traitement est de :\n",
    "- normaliser le texte : convertir tout en minuscule, ramener les mots à leur forme canonique,  ...\n",
    "- découper le texte en _tokens_ au niveau des espaces, des signes de ponctuation, ...\n",
    "- représenter le texte selon un formalisme mathématique pour permettre son analyse\n",
    "- finalement : préparer le texte à la mise en oeuvre de la tâche de TALN à accomplir.\n",
    "\n",
    "Dans cette partie, nous allons mettre en oeuvre quelques techniques de pré-traitements du texte et visualiser leurs intérêts. Pour cela, nous allons nous appuyer sur la bibliothèque de TALN _Spacy_ (https://spacy.io/) qui offre de nombreuses fonctionnalités pour la mise en place de traitement du langage en milieu industriel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des bibliothèques logicielles et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # Bibliothèque de TALN\n",
    "from spacy.lang.en.stop_words import STOP_WORDS # liste des mots vides en anglais\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop # liste des mots vides en français\n",
    "from spacy import displacy # visualisation des résultats\n",
    "import pandas as pd # manipulation de données\n",
    "import re # mise en oeuvre d'expression régulières\n",
    "import plotly.express as px # création de graphiques et visuels\n",
    "import ast # à ignorer pour ce TP\n",
    "\n",
    "# Téléchargement de modèles entraînés de TALN pour l'anglais et le français. *Attention* : nécessite Internet et peut prendre qques minutes.\n",
    "#!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# Chargement du modèle de TALN Français.\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "nlp.max_length = 400000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenisation consiste à découper une phrase en mots ou _tokens_. Le découpage se fait généralement au niveau des espaces, des signes de ponctuation et des retours à la ligne. \n",
    "\n",
    "Avant d'exploiter le jeu de données créé précédemment, nous allons travailler sur le texte de taille réduite ci-dessous (extrait de Wikipedia) afin de comprendre les algorithmes mis en oeuvre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Naval Group est un groupe industriel français spécialisé dans la construction navale de défense. \n",
    "Le groupe emploie 15 792 personnes en 2020 à travers dix-huit pays. \n",
    "Société de droit privé détenue principalement à hauteur de 62,49 % par l’État français et de 35 % par Thales, \n",
    "Naval Group est, depuis 2017, l’héritière des arsenaux français et de la Direction des constructions et armes navales (DCAN), \n",
    "devenue la Direction des constructions navales (DCN) en 1991 et DCNS en 2007 (le « S » ajouté pour la notion de système et de service). \n",
    "Depuis 2021, le groupe se recentre sur ses activités navales. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous est une fonction permettant d'afficher les tokens les plus fréquent dans une liste de tokens. On l'utilisera par la suite pour visualiser le résultat de différentes tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_frequent_tokens(tokens, max=50):\n",
    "    \"\"\" Fonction permettant d'afficher une liste de tokens sous la forme d'un graphique barre.\"\"\"\n",
    "    uniq_tokens = set(tokens)\n",
    "    token_count = {}\n",
    "    for token in uniq_tokens:\n",
    "        count = tokens.count(token)\n",
    "        token_count[token] = count\n",
    "    s = pd.Series(token_count).sort_values(ascending=False).head(max)\n",
    "    fig = px.bar(s)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tokenisation simple\n",
    "La fonction ci-dessous permet de tokeniser un texte de manière (un peu trop) simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(nlp, text):\n",
    "    \"\"\"Fonction retournant la liste des tokens dans le texte `text` passé en paramètre\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : appliquez la fonction de tokenisation au texte (variable `text`) et affichez le résultat. Le premier paramètre de la fonction est l'objet `nlp` défini plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] # TODO : remplacer le tableau vide [] l'appel de la fonction tokenize().\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : en utilisant la fonction `show_most_frequent_tokens`, afficher la liste des tokens les plus fréquent dans le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_frequent_tokens([]) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenisation a bien été réalisée mais elle comporte plusieurs écueils :\n",
    "- les signes de ponctuation sont considérés comme des tokens ;\n",
    "- les caractères spéciaux (`\\n` : caractère de retour à la ligne) sont aussi considérés comme des tokens ;\n",
    "- les mots ne sont pas normalisés (majuscules/minuscules ; pluriel/singulier ; ...)\n",
    "\n",
    "Améliorons notre tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tokenisation avec normalisation des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "special_chars = ['\\n', '\\r', '»', '«']\n",
    "\n",
    "forbidden_words = punctuations + special_chars + list(fr_stop)\n",
    "\n",
    "def tokenize_with_normalization(nlp, text):\n",
    "    \"\"\"Fonction retournant la liste des tokens normalisés dans le texte `text` passé en paramètre.\"\"\"\n",
    "    text_normalized = text.lower() # conversion du texte en minuscules.\n",
    "    doc = nlp(text_normalized)\n",
    "    tokens = [token.text for token in doc if token.text not in forbidden_words] # suppression des caractères spéciaux et des mots vides.\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : appliquez la fonction de tokenisation avec normalisation au texte `text` et affichez le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_normalized = [] # TODO\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : en utilisant la fonction `show_most_frequent_tokens`, afficher la liste des tokens les plus fréquent dans le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_frequent_tokens([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est plus cohérent mais il faudrait maintenant regrouper les termes ayant le même sens. Par exemple `navale` et `navales` véhiculent le même concept, ils devraient donc être associés au même token.\n",
    "\n",
    "Pour cela nous allons utiliser la _lemmatisation_, principe qui consiste à calculer le lemme d'un mot, c'est à dire une forme canonique. Pour un nom par exemple, il s'agit de sa forme au masculin singulier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Normalisation par lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "special_chars = ['\\n', '\\r', '»', '«']\n",
    "\n",
    "forbidden_words = punctuations + special_chars + list(fr_stop)\n",
    "\n",
    "def tokenize_with_lemmatization(nlp, text):\n",
    "    \"\"\"Fonction retournant la liste des tokens lemmatisés dans le texte `text` passé en paramètre.\"\"\"\n",
    "    text = re.sub('\\n','',text).lower()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in forbidden_words and len(token.lemma_) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : appliquez la fonction de tokenisation avec lemmatisation au texte (variable `text`) et affichez le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lemmatized = [] # TODO\n",
    "print(tokens_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : en utilisant la fonction `show_most_frequent_tokens`, afficher la liste des tokens les plus fréquents dans le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_frequent_tokens([]) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est maintenant plus cohérent : \n",
    "- les termes les plus fréquents sont _naval_, _construction_ : logique ;\n",
    "- les mots partageant un même lemme ont été regroupés ensemble.\n",
    "\n",
    "Il reste tout de même quelques imperfections : \n",
    "- on aurait aimé que les mots composés soient dans un même token. Par exemple `Naval Group` ne doit pas être découpé ;\n",
    "- `thales` a été réduit en `thale`.\n",
    "\n",
    "Des techniques existent pour réduire les imperfections de l'approche : l'extraction des entités nommées que nous verrons en Partie 3 ou l'analyse morpho-syntaxique que nous allons voir maintenant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse morpho-syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse morphosyntaxique consiste à catégoriser les mots en leur affectant une étiquette indiquant si le mot est un `nom`, un `verbe`, un `adjectif` etc. Cette technique permet de mieux comprendre le texte, d'interpréter une partie du sens des mots, de trouver les relations entre les mots et / ou de focaliser l'analyse uniquement sur une catégorie particulière de mots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_morpho_syntax_analysis(nlp, text):\n",
    "    \"\"\"Fonction retournant la liste des tokens et leur étiquette grammaticale.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [(token.text, token.pos_) for token in doc if token.lemma_ not in forbidden_words]\n",
    "    displacy.render(doc, style=\"dep\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : appliquez la fonction de tokenisation avec analayse morpho syntaxique au texte (variable `text`) et affichez le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_morpho_syntax = [] # TODO\n",
    "print(tokens_morpho_syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, créons une fonction de tokenisation pour ne garder que les noms propres `PROPN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_morpho_syntax_analysis_propn(nlp, text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in forbidden_words and  token.pos_ == 'PROPN']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : modifier le tokenizer ci-dessous pour ne conserver que le noms propres : `token.pos_ == 'PROPN'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_proper_nouns = [] # TODO\n",
    "show_most_frequent_tokens(tokens_proper_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec ces techniques, nous commençons à parvenir à extraire des informations intéressantes du texte !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application au jeu de données pour trouver les mots les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tester notre pré-traitement, nous allons charger le jeu de données vue en première partie et y appliquer la tokenisation.\n",
    "\n",
    "Pour simplifier la tâche, nous allons fusionner l'ensemble des documents pour trouver les termes les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du jeu de données enregistré à la partie 1.\n",
    "f = open(\"./dataset_processed/dataset.json\", \"r\")\n",
    "documents_raw = f.read()\n",
    "documents = ast.literal_eval(documents_raw)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des documents pour réaliser l'analyse.\n",
    "big_text = ' '.join([doc['content'] for doc in documents.values()])\n",
    "big_text = big_text[0:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : appliquez la fonction la fonction de tokenisation au texte `big_text` de votre choix puis afficher les tokens les plus fréquents à l'aide de la fonction `show_most_frequent_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] # TODO\n",
    "show_most_frequent_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie , nous avons comment découper le textes en Token et l'importance de la normalisation pour obtenir une liste cohérente de tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c80391f1238b45698e2dd8cb3d47fccae97762fdc20421908c45a9aed68776a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
